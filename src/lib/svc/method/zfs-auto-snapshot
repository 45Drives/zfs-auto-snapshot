#!/usr/bin/ksh

#
# CDDL HEADER START
#
# The contents of this file are subject to the terms of the
# Common Development and Distribution License, Version 1.0 only
# (the "License").  You may not use this file except in compliance
# with the License.
#
# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
# or http://www.opensolaris.org/os/licensing.
# See the License for the specific language governing permissions
# and limitations under the License.
#
# When distributing Covered Code, include this CDDL HEADER in each
# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
# If applicable, add the following below this CDDL HEADER, with the
# fields enclosed by brackets "[]" replaced with your own identifying
# information: Portions Copyright [yyyy] [name of copyright owner]
#
# CDDL HEADER END
#
# Copyright 2008 Sun Microsystems, Inc.  All rights reserved.
# Use is subject to license terms.
#


#
# This SMF method takes snapshots periodically of a zfs filesystem, with
# options to allow the user to keep a limited number of snapshots, or snapshot
# all child datasets. More documentation available at
# http://blogs.sun.com/timf
#
# The service will move itself into maintenance if it's unable to take a
# snapshot, destroy a snapshot as per the snapshot retention policy, unable to
# zfs send a dataset (if configured) or is unable to create or update the cron
# job.
#



# For interested developers, the main functions here, are schedule_snapshots,
# unschedule_snapshots and take_snapshot : the exit conditions from these
# functions check the state of the service before returning an appropriate
# value. The check_failure method is responsible for checking error codes from
# subprocesses, and when called with a non-zero argument, will degrade the
# service, and log an appropriate error message.


. /lib/svc/share/smf_include.sh

result=$SMF_EXIT_OK


# A prefix we use on all snapshot created by this script.
# See the definition of $SNAPNAME in the take_snapshot()
# function for more information.
PREFIX="zfs-auto-snap"

# A separator character for date strings, and to delimit
# snapshot label names. Needed because apparantly Samba
# clients can get confused by colons. Who knew?
SEP=":"

# This variable gets set to the restarter/logfile property
# whenever we have $FMRI defined. Used by the print_log and
# print_note functions below for all output, it's definied
# by the schedule_snapshots take_snapshots and unschedule_snapshots
# methods.
LOG=""


# Determine whether this invocation of the script can use
# the recursive snapshot feature in some versions of ZFS.
# a null string in this variable says we don't.
HAS_RECURSIVE=$(zfs snapshot 2>&1 | fgrep -e '-r')


function get_pair {
        NAME=$1
        shift 2
        echo "${NAME}=\"$@\""
        echo "export ${NAME}"
}

# A function to pull in the variables from the FMRI given
# as the first argument.
function zfs_smf_props {

IFS="
"
SMF_PROPS="$(svcprop -t -p zfs $1 |\
            sed -e 's#zfs/fs-name#zfs/fs_name#g' \
        -e 's#zfs/backup-lock#zfs/backup_lock#g' \
        -e 's#zfs/snapshot-children#zfs/snapshot_children#g' \
        -e 's#zfs/backup-save-cmd#zfs/backup_save_cmd#g' \
        -e 's#zfs/##g' -e 's/$/,/g')"
IFS=,
for line in $SMF_PROPS ; do
        IFS=' 	
'
        eval $(get_pair $line)
done
}


# this function validates the properties in the FMRI passed to it, then
# calls a function to create cron job that schedules a snapshot schedule based
# on the properties set in the service instance.
# $1 is assumed to be a valid FMRI
function schedule_snapshots {

	typeset FMRI=$1
	zfs_smf_props $FMRI 
	# FIXME need work in here to actually validate the FMRI props
	typeset FILESYS="$fs_name"
	typeset INTERVAL="$interval"
	typeset PERIOD="$period"
	typeset OFFSET="$offset"
	typeset STATE=0	

	typeset BACKUP="$backup"
	typeset BACKUP_SAVE_CMD="$backup_save_cmd"

	case $BACKUP in
	'full' | 'incremental' )
	    if [ -z "${BACKUP_SAVE_CMD}" ] ; then
	        check_failure 1 \
		    "Backup requested, but no backup command specified."
	    fi
	;;
	esac

	# for now, we're forcing the offset to be 0 seconds.
	typeset OFFSET=0

	case $FILESYS in
	"//")
		;;
	"##" )
		;;
	*)
		# validate the filesystem
		zfs list $FILESYS 2>&1 > /dev/null
		check_failure $? "ZFS filesystem does not exist!"
		;;
	esac

	# remove anything that's there at the moment
	unschedule_snapshots $FMRI		
	check_missed_snapshots $INTERVAL $PERIOD $FMRI

	add_cron_job $INTERVAL $PERIOD $OFFSET $FMRI
	
	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi	
	return $STATE
}


#
# Adding a cron job that runs exactly every x time-intervals is hard to do
# properly.
#
# For now, what I'm doing, is dividing the interval up into x bite chunks
# and running the cron job that often. The problem comes where the interval
# doesn't evenly divide up into x, leaving us taking to many, or too
# few snapshots at the edge of time intervals.
#
# A new implementation of cron would be nice, but until then, we'll
# just live with this.
#
function add_cron_job { # $INTERVAL $PERIOD $OFFSET $FMRI

	typeset INTERVAL=$1
	typeset PERIOD=$2
	typeset OFFSET=$3
	typeset FMRI=$4

	case $INTERVAL in
	'minutes')
	    TIMES=$(get_divisor 0 59 $PERIOD)
	    ENTRY="$TIMES * * * *"		
	;;

	'hours')
	    TIMES=$(get_divisor 0 23 $PERIOD)
	    ENTRY="0 $TIMES * * *"
	;;

	'days')
	    TIMES=$(get_divisor 1 31 $PERIOD)
	    ENTRY="0 0 $TIMES * *"
	;;

	'months')
	    TIMES=$(get_divisor 1 12 $PERIOD)
	    ENTRY="0 0 1 $TIMES *"
	;;

	'none')
	    return 0
	;;
	esac

	# Since we may have multiple instances all trying to start at
	# the same time, we need some form of locking around crontab.
 	# Normally we'd be able to get SMF to manage this, by defining dependencies -
	# but I'm not sure there's a way to prevent it from starting two instances
	# at the same time (without requiring users to explicitly state dependencies
	# and change them each time new instances are added)

	# This isn't perfect (eg. if someone else is running crontab at the
	# same time as us, we'll fail) but it'll do for now.
	LOCK_OWNED="false"
	while [ "$LOCK_OWNED" == "false" ] ; do
	    mkdir /tmp/zfs-auto-snapshot-lock > /dev/null 2>&1
	    if [ $? -eq 0 ] ; then
	        LOCK_OWNED=true
	    else
	        sleep 1
	    fi
	done

	# adding a cron job is essentially just looking for an existing entry,
	# removing it, and appending a new one. Neato.
	crontab -l | grep -v "/lib/svc/method/zfs-auto-snapshot $FMRI$" \
	    > /tmp/saved-crontab.$$

	echo "${ENTRY} /lib/svc/method/zfs-auto-snapshot $FMRI" \
	    >> /tmp/saved-crontab.$$

	crontab /tmp/saved-crontab.$$
	check_failure $? "Unable to add cron job!"
	
	# release our lock
	rm -rf /tmp/zfs-auto-snapshot-lock
	rm /tmp/saved-crontab.$$
	return 0
}



# this function removes a cron job was taking snapshots of $FMRI
# $1 is assumed to be a valid FMRI
function unschedule_snapshots {

	typeset FMRI=$1
	typeset INTERVAL=$interval

	# interval set to 'none' means we don't want cron jobs
	if [ "$INTERVAL" == "none" ]; then
	    return 0
	fi

	# See notes on $LOCK_OWNED variable in function add_cron_job
        LOCK_OWNED="false"
        while [ "$LOCK_OWNED" == "false" ]; do
	    mkdir /tmp/zfs-auto-snapshot-lock > /dev/null 2>&1
	    if [ $? -eq 0 ] ; then
	        LOCK_OWNED=true
	    else
	        sleep 1
	    fi
	done

	crontab -l | grep -v "/lib/svc/method/zfs-auto-snapshot $FMRI$" \
	    > /tmp/saved-crontab.$$

	crontab /tmp/saved-crontab.$$
	check_failure $? "Unable to unschedule snapshots for $FMRI"

	rm -rf /tmp/zfs-auto-snapshot-lock
	rm /tmp/saved-crontab.$$

	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi
}


# This function is intended to be called on service start. It checks to see
# if the last snapshot was taken more than <frequency> <intervals> ago,
# and if that's the case, takes a snapshot immediatedly.
function check_missed_snapshots { # $INTERVAL $PERIOD $FMRI
	set -x

	typeset INTERVAL=$1
	typeset PERIOD=$2
	typeset FMRI=$3
	typeset FILESYS=$fs_name
	typeset LABEL=$label

	case "$FILESYS" in
	"//")
		FILESYS=$(get_snapshot_datasets $LABEL $SNAP_CHILDREN)	
		;;
	"##")
		FILESYS=$(get_nonexcluded_datasets $LABEL)
		;;
	esac

	
	if [ "$LABEL" != "\"\"" ] ; then
		LABEL="${SEP}${LABEL}"
	else
		LABEL=""
	fi

	# check to see if there are any filesystems
	if [ -z "$FILESYS" ] ; then
		return 0
	fi

	# only interested in the first filesystem, assuming they
	# all have a similar creation date.
	set -A fs $FILESYS
	LAST_SNAPSHOT=$(zfs list -H -o name -r -t snapshot ${fs[0]} \
		| grep "${fs[0]}@${PREFIX}${LABEL}" | tail -1)

	# if we've never taken a snapshot, do nothing
	if [ -z "$LAST_SNAPSHOT" ] ; then
		return 0
	fi

	LAST_SNAP_TIME=$(zfs get -H -p -o value creation $LAST_SNAPSHOT)
	LAST_SNAP_TIME_HUMAN=$(zfs get -H -o value creation $LAST_SNAPSHOT)
	NOW=$(perl -e 'print time;')

	# slightly incorrect time accounting here, but good enough.
	MINUTE_S=60
	HOUR_S=$(( $MINUTE_S * 60 ))
	DAY_S=$(( $HOUR_S * 24 ))
	MONTH_S=$(( $DAY_S * 30 ))
	
	case $INTERVAL in
	"minutes")
		MULTIPLIER=$MINUTE_S
	;;
	"hours")
		MULTIPLIER=$HOUR_S
	;;
	"days")
		MULTIPLIER=$DAY_S
	;;
	"none")
		return 0
	;;
	"*")
		print_log "WARNING - unknown interval encountered in check_missed_snapshots!"
		return 1
	esac
	PERIOD_S=$(( $MULTIPLIER * $PERIOD ))
	AGO=$(( $NOW - $LAST_SNAP_TIME ))
	if [ $AGO -gt $PERIOD_S ] ; then
		print_log "Last snapshot for $FMRI taken on $LAST_SNAP_TIME_HUMAN"
		print_log "which was greater than the $PERIOD $INTERVAL schedule. Taking snapshot now."
		take_snapshot $FMRI
	fi
}

# This function actually takes the snapshot of the filesystem.
# $1 is assumed to be a valid FMRI
function take_snapshot {
	# want this to be global, used by check_failure
	FMRI=$1
	zfs_smf_props $FMRI

	export LOG=$log
	typeset DATE=$(date +%F-%H${SEP}%M${SEP}%S)
	typeset FILESYS="$fs_name"
	typeset KEEP=$keep
	typeset SNAP_CHILDREN=$snapshot_children

	typeset BACKUP=$backup

	typeset STATE=0

	# an identifier allows us to setup multiple snapshot schedules
	# per filesystem - so we append a <sep><label> token if the user has
	# requested one, which then gets used in the SNAPNAME.  SMF
	# returns the value '""' for the empty string to differentiate
	# between an unset property, and a set-but-empty property.
	# Shocking, I know.
	typeset LABEL="$label"
	
	# the "//" filesystem is special. We use it as a keyword
	# to determine whether to poll the ZFS "com.sun:auto-snapshot:${LABEL}"
	# user property which specifies which datasets should be snapshotted
	# and under which "label" - a set of default service instances that
	# snapshot at defined periods (daily, weekly, monthly, every 15 mins)

	# the "##" filesystem is also special. It takes snapshots of
	# all datasets (non-recursively) *except* those marked with the tag
	# "com.sun:auto-snapshot:${LABEL}" = false. We necessarily ignore
	# the SNAP_CHILDREN setting in this case, as that could result in
	# us inadvertently taking snapshots of a child dataset under the
	# parent's lack of "com.sun:auto-snapshot:${LABEL}" = false tag.
	
	case "$FILESYS" in
	"//")
		FILESYS=$(get_snapshot_datasets $LABEL $SNAP_CHILDREN)
		;;
	"##")
		FILESYS=$(get_nonexcluded_datasets $LABEL)
		SNAP_CHILDREN=false
		;;
	esac	
		
	if [ "$LABEL" != "\"\"" ] ; then
	    LABEL="${SEP}${LABEL}"
	else
	    LABEL=""
	fi

	# A flag for whether we're running in verbose mode or not
	VERBOSE="$verbose"

	typeset SNAPNAME="${PREFIX}${LABEL}-${DATE}"
	
	# Determine whether we should avoid scrubbing
	typeset AVOIDSCRUB=$avoidscrub

	# prune out the filesystems that are on pools currently being
	# scrubbed or resilvered. There's a risk that a scrub/resilver
	# will be started just after this check completes, but there's
	# also the risk that a running scrub will complete just after this
	# check. Life's hard.
	if [ "$AVOIDSCRUB" == "true" ] ; then
	    # a cache of the pools that are known not to be scrubbing
	    NOSCRUBLIST=""

	    # Create a list of filesystems scheduled for snapshots
	    # that are *not* on pools that are being scrubbed/resilvered
	    for fs in $FILESYS ; do
		POOL=$(echo $fs | cut -d/ -f1)
		if is_scrubbing $POOL "$NOSCRUBLIST" ; then
		    print_log "Pool containing $fs is being scrubbed/resilvered."
		    print_log "Not taking snapshots for $fs."
		else
		    NOSCRUBLIST="$POOL $NOSCRUBLIST"
		    NOSCRUBFILESYS="$NOSCRUBFILESYS $fs"
		fi
	    done		
	    FILESYS="$NOSCRUBFILESYS"
	fi

	# walk each of the filesystems specified
	for fs in $FILESYS ; do
	    # Ok, now say cheese! If we're taking recursive snapshots,
	    # walk through the children, destroying old ones if required.
	    if [ "${SNAP_CHILDREN}" == "true" ] ;  then

		if [ -z "${HAS_RECURSIVE}" ] ; then
	   	    for child in $(zfs list -r -H -o name -t filesystem,volume $fs) ; do
		        destroy_older_snapshots $child $KEEP $LABEL
		        print_note "Taking snapshot $child@$SNAPNAME"
		        zfs snapshot $child@$SNAPNAME
			check_failure $? "Unable to take snapshot $child@$SNAPNAME."
		    done
		else
		    destroy_older_snapshots $fs $KEEP $LABEL $HAS_RECURSIVE
	            print_note "Taking recursive snapshot $fs@$SNAPNAME"
	            zfs snapshot -r $fs@$SNAPNAME
		    check_failure $? "Unable to take recursive snapshots of $fs@$SNAPNAME."
	        fi
	
	    else
	         destroy_older_snapshots $fs $KEEP $LABEL
		 print_note "Taking snapshot $fs@$SNAPNAME"
		 zfs snapshot $fs@$SNAPNAME
		 check_failure $? "Unable to take snapshot $fs@$SNAPNAME."
	    fi

	    # If the user has asked for backups, go ahead and do this.
	    if [ "${BACKUP}" != "none" ] ;  then
	        take_backup $fs $BACKUP "$LABEL" $FMRI
	        check_failure $? "Unable to backup filesystem $fs using \
		        $BACKUP backup strategy."
	    fi

	done
	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi
	return $STATE
}

# Given a filesystem name, and a limit of the number of snapshots we want,
# along with the identifier for this set of snapshots,
# we destroy all older snapshots of this filesystem whose names begin
# with the text "${PREFIX}${LABEL}". Note that here we destroy one more snapshot
# than the "keep" threshold - this is because in the context of calling this
# function, we're already creating one new auto-snapshot.
#
function destroy_older_snapshots {

	typeset FILESYS=$1
	typeset COUNTER=$2
	typeset LABEL=$3
	typeset HAS_RECURSIVE=$4

	if [ "${COUNTER}" == "all" ] ; then
	    return 0
	fi

	if [ -n "${HAS_RECURSIVE}" ] ; then
	    typeset RECURSIVE="-r"
	fi
	
	COUNTER=$(($COUNTER - 1))
	# walk through the snapshots, newest first, destroying older ones
	for snapshot in $(zfs list -r -t snapshot -H -o name $FILESYS \
		 | grep "$FILESYS@${PREFIX}${LABEL}" | sort -r) ; do

	    if [ $COUNTER -le 0 ] ; then
	        # using print_note, as this checks our $VERBOSE flag
	        print_note "$snapshot being destroyed ${RECURSIVE} as per \
 retention policy."
	        zfs destroy ${RECURSIVE} $snapshot
	        check_failure $? "Unable to destroy $snapshot"
	    else
	        # don't destroy this one			
	        COUNTER=$(($COUNTER - 1))
	    fi
	done
}

# Given the exit status of a command, an integer, 0 if the command completed
# without errors. If the command exited with errors, we degrade the
# state of this service into maintenance mode. We also log an error message
# as passed into this function.
#
function check_failure { # integer exit status, error message to display

	typeset RESULT=$1
	typeset ERR_MSG=$2

	if [ $RESULT -ne 0 ] ; then
	    print_log "Error: $ERR_MSG"
	    print_log "Moving service $FMRI to maintenance mode."
	    svcadm mark maintenance $FMRI
	fi

}


# A function we use to emit output. Right now, this goes to syslog via logger(1)
# but it would be much nicer to be able to print it to the svc log file for
# each individual service instance - tricky because we're being called from
# cron, most of the time and are detached from smf. Working around this by
# appending to the $LOG file
function print_log { # message to display
	logger -t zfs-auto-snap -p daemon.notice $*
	echo $(date) $* >> $LOG
}

# Another function to emit output, this time checking to see if the
# user has set the service into verbose mode, otherwise, we print nothing
function print_note { # mesage to display
	if [ "$VERBOSE" == "true" ] ; then
	    logger -t zfs-auto-snap -p daemon.notice $*
	    echo $(date) $* >> $LOG
	fi
}


# Given a range start, end and width of period, return a comma
# separated string of numbers within that range and conforming to
# that period. This isn't ideal, but it'll do for now.
#
function get_divisor { # start period, end period, width of period

	typeset START=$1
	typeset END=$2
	typeset WIDTH=$3
	typeset RANGE=$START
	typeset JUMP=$(( $RANGE + $WIDTH ))

	while [ $JUMP -le $END ] ; do
	    RANGE="$RANGE,$JUMP"
	    JUMP=$(( $JUMP + $WIDTH ))
	done
	
	echo $RANGE
}


# Given a filesytem name, and a backup type (currently "complete" or
# "incremental") along with an FMRI, we backup the filesystem - either
# from the latest snapshot that was taken, or by an incremental backup.
# Properties in the FMRI tell us what to do with the backup stream
#
function take_backup { # filesystem backup-type label fmri

    typeset FILESYS=$1
    typeset BACKUP=$2
    typeset LABEL=$3
    typeset FMRI=$4


    # obtain lock from fmri
    typeset LOCK=$(svcprop -p zfs/backup-lock $FMRI)
    if [ "$LOCK" != "unlocked" ]
    then
	# Unable to perform this backup due to an existing backup being
	# executed for this dataset. This would result in moving the
	# service to maintenance mode if we're doing incrementals, since
	# missing an incremental backup will result in the user being unable
	# to restore future incremental backups. This isn't so serious for
	# full backups.
        print_log "Unable to backup $FILESYS: $LOCK."

	if [ "$BACKUP" == "incremental" ]
	then
	     print_log "A lock prevented us from performing an incremental backup."
	     return 1
	else
	     print_log "Full backup not completed for $FMRI."
	     return 0
	fi
    else
 	# set our lock. (this isn't atomic, unfortunately :-( )
        svccfg -s $FMRI setprop zfs/backup-lock = astring: \
	"\"$BACKUP backup in progress by PID $$\""
	svcadm refresh $FMRI
    fi

    typeset BACKUP_SAVE_CMD="$(echo $backup_save_cmd | sed -e 's/\\//g')"
    typeset SNAP_CHILDREN=$snapshot_children
    typeset BACKUP_DATASETS=""

    # Determine how many datasets we have to backup
    if [ "$SNAP_CHILDREN" == "true" ] ; then
	BACKUP_DATASETS=$(zfs list -r -H -o name -t filesystem,volume $FILESYS)
    else
        # only one dataset to worry about here.
	BACKUP_DATASETS=$FILESYS
    fi

    # loop through the datasets, backing up each one.
    for dataset in $BACKUP_DATASETS ; do

      # An initial check of the input parameters, to see how we should proceed
      case $BACKUP in
        "incremental")
		# get the last two snapshots
		LAST_SNAP=$(zfs list -H -o name -r -t snapshot $dataset \
			    | grep "$dataset@${PREFIX}${LABEL}" | tail -1)

		PREV_SNAP=$(zfs list -H -o name -r -t snapshot $dataset \
			    | grep "$dataset@${PREFIX}${LABEL}" \
			    | tail -2 | head -1)

		if [ "$PREV_SNAP" == "$LAST_SNAP" ]
		then
		    print_log "Previous snap not found of $dataset, taking full backup."
		    BACKUP="full"
		fi
	;;
        "full")
 		LAST_SNAP=$(zfs list -H -o name -r -t snapshot $dataset \
			    | grep "$dataset@${PREFIX}${LABEL}" | tail -1)
	;;
        *)
		check_failure 1 "Unknown backup type $BACKUP"
		svccfg -s $FMRI setprop zfs/backup-lock = astring: "unlocked"
		svcadm refresh $FMRI
		return 1
	;;
       esac


       # Now perform the backup. Note that on errors, we'll immediately mark
       # the service as being in maintenance mode, however, backups will still
       # be attempted for other datasets in our list.
       case $BACKUP in
	"incremental")
	    print_note "Starting incr. ZFS send of differences between $PREV_SNAP and $LAST_SNAP."
	    zfs send -i $PREV_SNAP $LAST_SNAP | $BACKUP_SAVE_CMD
	    check_failure $? "Error performing incremental backup of $dataset."
	;;
	"full")
	    print_note "Starting ZFS send of $LAST_SNAP."
	    zfs send $LAST_SNAP | $BACKUP_SAVE_CMD
	    check_failure $? "Error performing full backup of $dataset."
	;;
       esac
   done
   print_note "Backups completed for $dataset."
   # Now we can release our lock
   svccfg -s $FMRI setprop zfs/backup-lock = astring: "unlocked"
   svcadm refresh $FMRI

}

# Get a list of filesystem we should snapshot. If snap_children is "true"
# then we don't list children that inherit the parent's property - we just look
# for locally set properties, and let "zfs snapshot -r" snapshot the children.
function get_snapshot_datasets { #LABEL #SNAP_CHILDREN

	typeset LABEL=$1
	typeset SNAP_CHILDREN=$2
	if [ "${SNAP_CHILDREN}" = "true" ] ; then
	    typeset FS=$(zfs get -s local -o name,value com.sun:auto-snapshot:$LABEL \
	        | grep true$ | awk '{print $1}')
	else
	    typeset FS=$(zfs list -t filesystem,volume \
		-o name,com.sun:auto-snapshot:$LABEL \
	        | grep true$ | awk '{print $1}')
	fi
	echo "$FS"
}

# Get a list of filesystems we should snapshot. We look for all filesystems
# and volumes that don't have a property com.sun:auto-snapshot:$LABEL
# set to false.
function get_nonexcluded_datasets {

	typeset LABEL=$1
	typeset FS=$(zfs list -H -t filesystem,volume \
	    -o name,com.sun:auto-snapshot:$LABEL \
	    | grep -v false$ | awk '{print $1}')
	echo "$FS"
}



# Determine if a pool is currently being scrubbed or resilvered.
# Return 0 if it is scrubbing/resilvering, 1 otherwise.

# The 2nd arg is a cache of pools known to be not scrubbing during this
# invocation of the script. This does risk a scrub starting mid-way through
# the script being started and us not checking for it - but if that's just
# happened, then restarting the scrub as a result of a snapshot being taken
# won't be too expensive.
function is_scrubbing { # POOL SCRUBLIST
	
	typeset POOL=$1
	typeset NOSCRUBLIST="$2"
	typeset SCRUBBING=""

	# see if we can avoid running zpool status, by checking for
	# the pool name in a known list of pools that were not scrubbing
	# the last time we checked.
	echo "$NOSCRUBLIST" | grep "$POOL " > /dev/null
	if [ $? -eq 0 ] ; then
	    return 1
	fi

	SCRUBBING=$(env LC_ALL=C zpool status $POOL | grep " in progress")
	if [ -z "$SCRUBBING" ] ; then
	    return 1
	else
	    return 0
	fi
}




# Here's the beginning of the main script. As we're a method script for SMF,
# we take start and stop arguments, and assume that the $SMF_FMRI value is being
# set. For start and stop, our task is to create a cron job that will take a
# snapshot of the specified filesystem.
#
# Without start | stop arguments, we assume we're being called from the cron job
# created above, where the argument is the FMRI containing properties we can
# consult to in order to actually take the snapshot.

if [ -n "${SMF_FMRI}" ] ; then
	zfs_smf_props $SMF_FMRI
	export LOG=${log}
fi

# $1 start | stop | an FMRI that we want to take snapshots of.
case "$1" in
'start')
	schedule_snapshots $SMF_FMRI
	if [ $? -eq 0 ] ; then
	    result=$SMF_EXIT_OK
	else
	    print_log "Problem taking snapshots for $SMF_FMRI"
	    result=$SMF_EXIT_ERR_FATAL
	fi
        ;;

'stop')
	unschedule_snapshots $SMF_FMRI
	if [ $? -eq 0 ] ; then
	    result=$SMF_EXIT_OK
	else
	    print_log "Problem taking snapshots for $SMF_FMRI"
	    result=$SMF_EXIT_ERR_FATAL
	fi
        ;;

# the default case, we actually call from the cron job itself that's
# executing this script, and do the job of taking snapshots.
*)
	SMF_FMRI=$1
	# are we being called with the correct argument (an FMRI) ?

	case $SMF_FMRI in
		svc:/*)
			take_snapshot $SMF_FMRI
			if [ $? -eq 0 ] ; then
			    result=$SMF_EXIT_OK
			else
			    result=$SMF_EXIT_ERR_FATAL
			fi
			;;
		*)
			# not logging these messages - we're assuming a curious
			# user has run the script from the command line.
			echo "Usage from SMF : zfs-auto-snapshot [start | stop]"
			echo "Usage from cron: zfs-auto-snapshot svc:/system/filesystem/zfs/auto-snapshot:instance"
			result=$SMF_EXIT_ERR_FATAL
			;;
		esac			
	;;

esac

exit $result
