#!/usr/bin/ksh

#
# CDDL HEADER START
#
# The contents of this file are subject to the terms of the
# Common Development and Distribution License, Version 1.0 only
# (the "License").  You may not use this file except in compliance
# with the License.
#
# You can obtain a copy of the license at usr/src/OPENSOLARIS.LICENSE
# or http://www.opensolaris.org/os/licensing.
# See the License for the specific language governing permissions
# and limitations under the License.
#
# When distributing Covered Code, include this CDDL HEADER in each
# file and include the License file at usr/src/OPENSOLARIS.LICENSE.
# If applicable, add the following below this CDDL HEADER, with the
# fields enclosed by brackets "[]" replaced with your own identifying
# information: Portions Copyright [yyyy] [name of copyright owner]
#
# CDDL HEADER END
#
# Copyright 2008 Sun Microsystems, Inc.  All rights reserved.
# Use is subject to license terms.
#


#
# This SMF method takes snapshots periodically of a zfs filesystem, with
# options to allow the user to keep a limited number of snapshots, or snapshot
# all child datasets. More documentation available at
# http://blogs.sun.com/timf
#
# The service will move itself into maintenance if it's unable to take a
# snapshot, destroy a snapshot as per the snapshot retention policy, unable to
# zfs send a dataset (if configured) or is unable to create or update the cron
# job.
#



# For interested developers, the main functions here, are schedule_snapshots,
# unschedule_snapshots and take_snapshot : the exit conditions from these
# functions check the state of the service before returning an appropriate
# value. The check_failure method is responsible for checking error codes from
# subprocesses, and when called with a non-zero argument, will degrade the
# service, and log an appropriate error message.


. /lib/svc/share/smf_include.sh

result=$SMF_EXIT_OK

export PATH=/usr/sbin:/usr/bin:${PATH}

# A prefix we use on all snapshot created by this script.
# See the definition of $SNAPNAME in the take_snapshot()
# function for more information.
PREFIX="zfs-auto-snap"

# A separator character for date strings, and to delimit
# snapshot label names. Needed because apparantly Samba
# clients can get confused by colons. Who knew?
SEP=":"

# This variable gets set to the restarter/logfile property
# whenever we have $FMRI defined. Used by the print_log and
# print_note functions below for all output, it's defined
# by the schedule_snapshots take_snapshots and unschedule_snapshots
# methods. Note that for take_snapshot LOG gets set to
# a file in the zfssnap role's home directory, as we don't own
# the SMF log. Start/stop logging goes to retarter/logfile,
# everything else goes in the zfssnap role's log.
LOG=""


# Determine whether this invocation of the script can use
# the recursive snapshot feature in some versions of ZFS.
# a null string in this variable says we don't.
HAS_RECURSIVE=$(zfs snapshot 2>&1 | fgrep -e '-r')

# OpenSolaris has no pfksh, so for now, we need to pfexec
# all calls to zfs that require privileges
PFZFS="pfexec zfs"

function get_pair {
        NAME=$1
        shift 2
        echo "${NAME}=\"$@\""
        echo "export ${NAME}"
}

# A function to pull in the variables from the FMRI given
# as the first argument.
function zfs_smf_props {

IFS="
"
SMF_PROPS="$(svcprop -t -p zfs -p restarter/logfile $1 |\
            sed -e 's#zfs/fs-name#zfs/fs_name#g' \
        -e 's#zfs/backup-lock#zfs/backup_lock#g' \
        -e 's#zfs/snapshot-children#zfs/snapshot_children#g' \
        -e 's#zfs/backup-save-cmd#zfs/backup_save_cmd#g' \
        -e 's#zfs/##g' -e 's#restarter/##g' -e 's/$/,/g')"
IFS=,
for line in $SMF_PROPS ; do
        IFS=' 	
'
        eval $(get_pair $line)
done
}


# this function validates the properties in the FMRI passed to it, then
# calls a function to create cron job that schedules a snapshot schedule based
# on the properties set in the service instance.
# $1 is assumed to be a valid FMRI
function schedule_snapshots {

	typeset FMRI=$1
	zfs_smf_props $FMRI 
	# functions we call need $LOG set.
	export LOG=$logfile

	# FIXME need work in here to actually validate the FMRI props
	typeset FILESYS="$fs_name"
	typeset INTERVAL="$interval"
	typeset PERIOD="$period"
	typeset OFFSET="$offset"
	typeset STATE=0	

	typeset BACKUP="$backup"
	typeset BACKUP_SAVE_CMD="$backup_save_cmd"

	case $BACKUP in
	'full' | 'incremental' )
	    if [ -z "${BACKUP_SAVE_CMD}" ] ; then
	        check_failure 1 \
		    "Backup requested, but no backup command specified."
	    fi
	;;
	esac

	# for now, we're forcing the offset to be 0 seconds.
	typeset OFFSET=0

	case $FILESYS in
	'//')
		;;
	*)
		# validate the filesystem
		zfs list $FILESYS 2>&1 > /dev/null
		check_failure $? "ZFS filesystem does not exist!"
		;;
	esac

	# remove anything that's there at the moment
	unschedule_snapshots $FMRI		
	check_missed_snapshots $INTERVAL $PERIOD $FMRI

	add_cron_job $INTERVAL $PERIOD $OFFSET $FMRI
	
	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi	
	return $STATE
}


#
# Adding a cron job that runs exactly every x time-intervals is hard to do
# properly.
#
# For now, what I'm doing, is dividing the interval up into x bite chunks
# and running the cron job that often. The problem comes where the interval
# doesn't evenly divide up into x, leaving us taking to many, or too
# few snapshots at the edge of time intervals.
#
# A new implementation of cron would be nice, but until then, we'll
# just live with this.
#
function add_cron_job { # $INTERVAL $PERIOD $OFFSET $FMRI

	typeset INTERVAL=$1
	typeset PERIOD=$2
	typeset OFFSET=$3
	typeset FMRI=$4

	case $INTERVAL in
	'minutes')
	    TIMES=$(get_divisor 0 59 $PERIOD)
	    ENTRY="$TIMES * * * *"		
	;;

	'hours')
	    TIMES=$(get_divisor 0 23 $PERIOD)
	    ENTRY="0 $TIMES * * *"
	;;

	'days')
	    TIMES=$(get_divisor 1 31 $PERIOD)
	    ENTRY="0 0 $TIMES * *"
	;;

	'months')
	    TIMES=$(get_divisor 1 12 $PERIOD)
	    ENTRY="0 0 1 $TIMES *"
	;;

	'none')
	    return 0
	;;
	esac

	# Since we may have multiple instances all trying to start at
	# the same time, we need some form of locking around crontab.
 	# Normally we'd be able to get SMF to manage this, by defining dependencies -
	# but I'm not sure there's a way to prevent it from starting two instances
	# at the same time (without requiring users to explicitly state dependencies
	# and change them each time new instances are added)

	# This isn't perfect (eg. if someone else is running crontab at the
	# same time as us, we'll fail) but it'll do for now.
	LOCK_OWNED="false"
	while [ "$LOCK_OWNED" == "false" ] ; do
	    mkdir /tmp/zfs-auto-snapshot-lock > /dev/null 2>&1
	    if [ $? -eq 0 ] ; then
	        LOCK_OWNED=true
	    else
	        sleep 1
	    fi
	done

	# adding a cron job is essentially just looking for an existing entry,
	# removing it, and appending a new one. Neato.
	crontab -l | grep -v "/lib/svc/method/zfs-auto-snapshot $FMRI$" \
	    > /tmp/saved-crontab.$$

	echo "${ENTRY} /lib/svc/method/zfs-auto-snapshot $FMRI" \
	    >> /tmp/saved-crontab.$$

	crontab /tmp/saved-crontab.$$
	check_failure $? "Unable to add cron job!"
	
	# release our lock
	rm -rf /tmp/zfs-auto-snapshot-lock
	rm /tmp/saved-crontab.$$
	return 0
}



# this function removes a cron job was taking snapshots of $FMRI
# $1 is assumed to be a valid FMRI
function unschedule_snapshots {

	typeset FMRI=$1
	typeset INTERVAL=$interval

	# interval set to 'none' means we don't want cron jobs
	if [ "$INTERVAL" == "none" ]; then
	    return 0
	fi

	# See notes on $LOCK_OWNED variable in function add_cron_job
        LOCK_OWNED="false"
        while [ "$LOCK_OWNED" == "false" ]; do
	    mkdir /tmp/zfs-auto-snapshot-lock > /dev/null 2>&1
	    if [ $? -eq 0 ] ; then
	        LOCK_OWNED=true
	    else
	        sleep 1
	    fi
	done

	crontab -l | grep -v "/lib/svc/method/zfs-auto-snapshot $FMRI$" \
	    > /tmp/saved-crontab.$$

	crontab /tmp/saved-crontab.$$
	check_failure $? "Unable to unschedule snapshots for $FMRI"

	rm -rf /tmp/zfs-auto-snapshot-lock
	rm /tmp/saved-crontab.$$

	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi
}


# This function is intended to be called on service start. It checks to see
# if the last snapshot was taken more than <frequency> <intervals> ago,
# and if that's the case, takes a snapshot immediatedly.
function check_missed_snapshots { # $INTERVAL $PERIOD $FMRI <repopulate cache>
	set -x

	typeset INTERVAL=$1
	typeset PERIOD=$2
	typeset FMRI=$3
	typeset FILESYS=$fs_name
	typeset LABEL=$label

	if [ -n $4 ] ; then
		typeset NO_CACHE_REPOPULATE=$4
	fi

	# // is special, in that we take snapshots based on user properties
	# so here, we get those properties, and call ourselves again, with
	# those values.
	case "$FILESYS" in
	"//")
		get_userprop_datasets	
		export snapshot_children=false
		export fs_name="$SINGLE_LIST"
		print_note "Checking for non-recursive missed // snapshots $SINGLE_LIST"
		check_missed_snapshots $INTERVAL $PERIOD $FMRI no_repopulate_cache

		export snapshot_children=true
		export fs_name="$RECURSIVE_LIST"
		print_note "Checking for recursive missed // snapshots $RECURSIVE_LIST"
		check_missed_snapshots $INTERVAL $PERIOD $FMRI no_repopulate_cache
			
		return 0
		;;
	esac

	
	if [ "$LABEL" != "\"\"" ] ; then
		LABEL="${SEP}${LABEL}"
	else
		LABEL=""
	fi

	# check to see if there are any filesystems
	if [ -z "$FILESYS" ] ; then
		return 0
	fi

	# only interested in the first filesystem, assuming they
	# all have a similar creation date.
	set -A fs $FILESYS
	LAST_SNAPSHOT=$(zfs list -H -o name -r -t snapshot ${fs[0]} \
		| grep "${fs[0]}@${PREFIX}${LABEL}" | tail -1)

	# if we've never taken a snapshot, do nothing
	if [ -z "$LAST_SNAPSHOT" ] ; then
		return 0
	fi

	LAST_SNAP_TIME=$(zfs get -H -p -o value creation $LAST_SNAPSHOT)
	LAST_SNAP_TIME_HUMAN=$(zfs get -H -o value creation $LAST_SNAPSHOT)
	NOW=$(perl -e 'print time;')

	# slightly incorrect time accounting here, but good enough.
	MINUTE_S=60
	HOUR_S=$(( $MINUTE_S * 60 ))
	DAY_S=$(( $HOUR_S * 24 ))
	MONTH_S=$(( $DAY_S * 30 ))
	
	case $INTERVAL in
	"minutes")
		MULTIPLIER=$MINUTE_S
	;;
	"hours")
		MULTIPLIER=$HOUR_S
	;;
	"days")
		MULTIPLIER=$DAY_S
	;;
	"none")
		return 0
	;;
	"*")
		print_log "WARNING - unknown interval encountered in check_missed_snapshots!"
		return 1
	esac
	PERIOD_S=$(( $MULTIPLIER * $PERIOD ))
	AGO=$(( $NOW - $LAST_SNAP_TIME ))
	if [ $AGO -gt $PERIOD_S ] ; then
		print_log "Last snapshot for $FMRI taken on $LAST_SNAP_TIME_HUMAN"
		print_log "which was greater than the $PERIOD $INTERVAL schedule. Taking snapshot now."
		take_snapshot $FMRI $NO_CACHE_REPOPULATE
	fi
}

# This function actually takes the snapshot of the filesystem.
# $1 is assumed to be a valid FMRI. $2 if non-null makes us skip
# populating the SMF property cache - used only by the special
# // snapshot type.
function take_snapshot {
	# want this to be global, used by check_failure
	FMRI=$1
	NO_CACHE_REPOPULATE=$2

	if [ -z "$NO_CACHE_REPOPULATE" ] ; then
		zfs_smf_props $FMRI
	fi

	# When taking snapshots, because we're running as a role
	# and can't redirect our output through SMF, we don't have
	# permissions to log to /var/svc/log, so we instead log
	# to the role's home directory.
	LOG_BASE=$(basename $logfile)
	export LOG="$HOME/$LOG_BASE"

	typeset DATE=$(date +%F-%H${SEP}%M)
	typeset FILESYS="$fs_name"
	typeset KEEP=$keep
	typeset SNAP_CHILDREN=$snapshot_children

	typeset BACKUP=$backup

	typeset STATE=0

	# an identifier allows us to setup multiple snapshot schedules
	# per filesystem - so we append a <sep><label> token if the user has
	# requested one, which then gets used in the SNAPNAME.  SMF
	# returns the value '""' for the empty string to differentiate
	# between an unset property, and a set-but-empty property.
	# Shocking, I know.
	typeset LABEL="$label"
	
	# the "//" filesystem is special. We use it as a keyword
	# to determine whether to poll the ZFS "com.sun:auto-snapshot:${LABEL}"
	# user property which specifies which datasets should be snapshotted
	# and under which "label" - a set of default service instances that
	# snapshot at defined periods (daily, weekly, monthly, every 15 mins)
	# Determine what these are, call ourselves again, then return.
	case "$FILESYS" in
	"//")
		# this populates two values SINGLE_LIST and RECURSIVE_LIST
		get_userprop_datasets $LABEL

		print_note "Taking non-recursive snapshots $SINGLE_LIST"
		export snapshot_children=false
		export fs_name="$SINGLE_LIST"
		take_snapshot $FMRI no_propcache_repopulate
		single_STATE=$?

		print_note "Taking recursive snapshots of $RECURSIVE_LIST"
		export snapshot_childrent=false
		export fs_name="$RECURSIVE_LIST"
		take_snapshot $FMRI no_propcache_repopulate
		recursive_STATE=$?
		return $single_STATE && $recursive_STATE
	;;
	esac	
		
	if [ "$LABEL" != "\"\"" ] ; then
	    LABEL="${SEP}${LABEL}"
	else
	    LABEL=""
	fi

	# A flag for whether we're running in verbose mode or not
	VERBOSE="$verbose"

	typeset SNAPNAME="${PREFIX}${LABEL}-${DATE}"
	
	# Determine whether we should avoid scrubbing
	typeset AVOIDSCRUB=$avoidscrub

	# prune out the filesystems that are on pools currently being
	# scrubbed or resilvered. There's a risk that a scrub/resilver
	# will be started just after this check completes, but there's
	# also the risk that a running scrub will complete just after this
	# check. Life's hard.
	if [ "$AVOIDSCRUB" == "true" ] ; then
	    # a cache of the pools that are known not to be scrubbing
	    NOSCRUBLIST=""

	    # Create a list of filesystems scheduled for snapshots
	    # that are *not* on pools that are being scrubbed/resilvered
	    for fs in $FILESYS ; do
		POOL=$(echo $fs | cut -d/ -f1)
		if is_scrubbing $POOL "$NOSCRUBLIST" ; then
		    print_log "Pool containing $fs is being scrubbed/resilvered."
		    print_log "Not taking snapshots for $fs."
		else
		    NOSCRUBLIST="$POOL $NOSCRUBLIST"
		    NOSCRUBFILESYS="$NOSCRUBFILESYS $fs"
		fi
	    done		
	    FILESYS="$NOSCRUBFILESYS"
	fi

	# walk each of the filesystems specified
	for fs in $FILESYS ; do
	    # Ok, now say cheese! If we're taking recursive snapshots,
	    # walk through the children, destroying old ones if required.
	    if [ "${SNAP_CHILDREN}" == "true" ] ;  then

		if [ -z "${HAS_RECURSIVE}" ] ; then
	   	    for child in $(zfs list -r -H -o name -t filesystem,volume $fs) ; do
		        destroy_older_snapshots $child $KEEP $LABEL
		        print_note "Taking snapshot $child@$SNAPNAME"
		        $PFZFS snapshot $child@$SNAPNAME
			check_failure $? "Unable to take snapshot $child@$SNAPNAME."
		    done
		else
		    destroy_older_snapshots $fs $KEEP $LABEL $HAS_RECURSIVE
	            print_note "Taking recursive snapshot $fs@$SNAPNAME"
	            $PFZFS snapshot -r $fs@$SNAPNAME
		    check_failure $? "Unable to take recursive snapshots of $fs@$SNAPNAME."
	        fi
	
	    else
	         destroy_older_snapshots $fs $KEEP $LABEL
		 print_note "Taking snapshot $fs@$SNAPNAME"
		 $PFZFS snapshot $fs@$SNAPNAME
		 check_failure $? "Unable to take snapshot $fs@$SNAPNAME."
	    fi

	    # If the user has asked for backups, go ahead and do this.
	    if [ "${BACKUP}" != "none" ] ;  then
	        take_backup $fs $BACKUP "$LABEL" $FMRI
	        check_failure $? "Unable to backup filesystem $fs using \
		        $BACKUP backup strategy."
	    fi

	done
	# finally, check our status before we return
	STATE=$(svcprop -p restarter/state $FMRI)
	if [ "${STATE}" == "maintenance" ] ; then
	    STATE=1
	else
	    STATE=0
	fi
	return $STATE
}

# Given a filesystem name, and a limit of the number of snapshots we want,
# along with the identifier for this set of snapshots,
# we destroy all older snapshots of this filesystem whose names begin
# with the text "${PREFIX}${LABEL}". Note that here we destroy one more snapshot
# than the "keep" threshold - this is because in the context of calling this
# function, we're already creating one new auto-snapshot.
#
function destroy_older_snapshots {

	typeset FILESYS=$1
	typeset COUNTER=$2
	typeset LABEL=$3
	typeset HAS_RECURSIVE=$4

	if [ "${COUNTER}" == "all" ] ; then
	    return 0
	fi

	if [ -n "${HAS_RECURSIVE}" ] ; then
	    typeset RECURSIVE="-r"
	fi
	
	COUNTER=$(($COUNTER - 1))
	# walk through the snapshots, newest first, destroying older ones
	for snapshot in $(zfs list -r -t snapshot -H -o name $FILESYS \
		 | grep "$FILESYS@${PREFIX}${LABEL}" | sort -r) ; do

	    if [ $COUNTER -le 0 ] ; then
	        # using print_note, as this checks our $VERBOSE flag
	        print_note "$snapshot being destroyed ${RECURSIVE} as per \
 retention policy."
	        $PFZFS destroy ${RECURSIVE} $snapshot
	        check_failure $? "Unable to destroy $snapshot" "NON_FATAL"
	    else
	        # don't destroy this one			
	        COUNTER=$(($COUNTER - 1))
	    fi
	done
}

# Given the exit status of a command, an integer, 0 if the command completed
# without errors. If the command exited with errors we degrade the
# state of this service into maintenance mode. If a 3rd argument is presented
# we don't degrade the service. We also log an error message as passed into
# this function.
#
function check_failure { # integer exit status, error message to display, be fatal

	typeset RESULT=$1
	typeset ERR_MSG=$2
	typeset NON_FATAL=$3

	if [ $RESULT -ne 0 ] ; then
	    print_log "Error: $ERR_MSG"
	    print_log "Moving service $FMRI to maintenance mode."
	    if [ -z "${NON_FATAL}" ] ; then
		print_log "Moving service $FMRI to maintenance mode."
	        svcadm mark maintenance $FMRI
	    fi
	fi

}


# A function we use to emit output. Right now, this goes to syslog via logger(1)
# but it would be much nicer to be able to print it to the svc log file for
# each individual service instance - tricky because we're being called from
# cron, most of the time and are detached from smf. Working around this by
# appending to the $LOG file
function print_log { # message to display
	logger -t zfs-auto-snap -p daemon.notice $*
	echo $(date) $* >> $LOG
}

# Another function to emit output, this time checking to see if the
# user has set the service into verbose mode, otherwise, we print nothing
function print_note { # mesage to display
	if [ "$VERBOSE" == "true" ] ; then
	    logger -t zfs-auto-snap -p daemon.notice $*
	    echo $(date) $* >> $LOG
	fi
}


# Given a range start, end and width of period, return a comma
# separated string of numbers within that range and conforming to
# that period. This isn't ideal, but it'll do for now.
#
function get_divisor { # start period, end period, width of period

	typeset START=$1
	typeset END=$2
	typeset WIDTH=$3
	typeset RANGE=$START
	typeset JUMP=$(( $RANGE + $WIDTH ))

	while [ $JUMP -le $END ] ; do
	    RANGE="$RANGE,$JUMP"
	    JUMP=$(( $JUMP + $WIDTH ))
	done
	
	echo $RANGE
}


# Given a filesytem name, and a backup type (currently "complete" or
# "incremental") along with an FMRI, we backup the filesystem - either
# from the latest snapshot that was taken, or by an incremental backup.
# Properties in the FMRI tell us what to do with the backup stream
#
function take_backup { # filesystem backup-type label fmri

    typeset FILESYS=$1
    typeset BACKUP=$2
    typeset LABEL=$3
    typeset FMRI=$4


    # obtain lock from fmri
    typeset LOCK=$(svcprop -p zfs/backup-lock $FMRI)
    if [ "$LOCK" != "unlocked" ]
    then
	# Unable to perform this backup due to an existing backup being
	# executed for this dataset. This would result in moving the
	# service to maintenance mode if we're doing incrementals, since
	# missing an incremental backup will result in the user being unable
	# to restore future incremental backups. This isn't so serious for
	# full backups.
        print_log "Unable to backup $FILESYS: $LOCK."

	if [ "$BACKUP" == "incremental" ]
	then
	     print_log "A lock prevented us from performing an incremental backup."
	     return 1
	else
	     print_log "Full backup not completed for $FMRI."
	     return 0
	fi
    else
 	# set our lock. (this isn't atomic, unfortunately :-( )
        svccfg -s $FMRI setprop zfs/backup-lock = astring: \
	"\"$BACKUP backup in progress by PID $$\""
	svcadm refresh $FMRI
    fi

    typeset BACKUP_SAVE_CMD="$(echo $backup_save_cmd | sed -e 's/\\//g')"
    typeset SNAP_CHILDREN=$snapshot_children
    typeset BACKUP_DATASETS=""

    # Determine how many datasets we have to backup
    if [ "$SNAP_CHILDREN" == "true" ] ; then
	BACKUP_DATASETS=$(zfs list -r -H -o name -t filesystem,volume $FILESYS)
    else
        # only one dataset to worry about here.
	BACKUP_DATASETS=$FILESYS
    fi

    # loop through the datasets, backing up each one.
    for dataset in $BACKUP_DATASETS ; do

      # An initial check of the input parameters, to see how we should proceed
      case $BACKUP in
        "incremental")
		# get the last two snapshots
		LAST_SNAP=$(zfs list -H -o name -r -t snapshot $dataset \
			    | grep "$dataset@${PREFIX}${LABEL}" | tail -1)

		PREV_SNAP=$(zfs list -H -o name -r -t snapshot $dataset \
			    | grep "$dataset@${PREFIX}${LABEL}" \
			    | tail -2 | head -1)

		if [ "$PREV_SNAP" == "$LAST_SNAP" ]
		then
		    print_log "Previous snap not found of $dataset, taking full backup."
		    BACKUP="full"
		fi
	;;
        "full")
 		LAST_SNAP=$(zfs list -H -o name -r -t snapshot $dataset \
			    | grep "$dataset@${PREFIX}${LABEL}" | tail -1)
	;;
        *)
		check_failure 1 "Unknown backup type $BACKUP"
		svccfg -s $FMRI setprop zfs/backup-lock = astring: "unlocked"
		svcadm refresh $FMRI
		return 1
	;;
       esac


       # Now perform the backup. Note that on errors, we'll immediately mark
       # the service as being in maintenance mode, however, backups will still
       # be attempted for other datasets in our list.
       case $BACKUP in
	"incremental")
	    print_note "Starting incr. ZFS send of differences between $PREV_SNAP and $LAST_SNAP."
	    $PFZFS send -i $PREV_SNAP $LAST_SNAP | $BACKUP_SAVE_CMD
	    check_failure $? "Error performing incremental backup of $dataset."
	;;
	"full")
	    print_note "Starting ZFS send of $LAST_SNAP."
	    $PFZFS send $LAST_SNAP | $BACKUP_SAVE_CMD
	    check_failure $? "Error performing full backup of $dataset."
	;;
       esac
   done
   print_note "Backups completed for $dataset."
   # Now we can release our lock
   svccfg -s $FMRI setprop zfs/backup-lock = astring: "unlocked"
   svcadm refresh $FMRI

}


# Given a sorted list of filesystems, determine whether any of the
# listed filesystems are redundant
# eg. for tank/other tank/foo tank/other/bar tank/foo/bar
# we only need to snapshot tank/other and tank/foo
function narrow_recursive_filesystems {
	# for each filesystem in the list, get each of it's ancestors
	# if any of the ancestors is already in the list, don't add it,
	# otherwise, do.
	typeset LIST=""
	for ds in $@ ; do
		ANCESTOR_IN_LIST=""
		ancestor=$(dirname $ds)
		while [ $ancestor != "." ] ; do
			if echo $LIST | fgrep $ancestor// > /dev/null ; then
				ANCESTOR_IN_LIST=true
			fi
			ancestor=$(dirname $ancestor)
		done
		if [ -z "${ANCESTOR_IN_LIST}" ] ; then
			LIST="${LIST} ${ds}//"
		fi
	done
	echo ${LIST} | sed -e 's#//##g'
}

function can_recursive_snapshot {
        typeset ds=$1
        if egrep "$ds/"\|"$ds	" $EXCLUDE > /dev/null; then
                # we can't recursively snapshot $ds because
                # it's excluded or is in the path to an excluded dataset
                return 1
        else
                return 0
        fi
}

function is_excluded {
        typeset ds=$1
        if egrep "$ds	" $EXCLUDE > /dev/null ; then
                return 0
        else
                return 1
        fi
}

# This builds two lists of datasets - RECURSIVE_LIST and SINGLE_LIST
# based on the value of ZFS user properties com.sun:auto-snapshot and
# com.sun:auto-snapshot:${LABEL}, the first argument to this script.
# RECURSIVE_LIST is a list of datasets that can be snapshotted with -r
# SINGLE_LIST is a list of datasets to snapshot individually.
#
function get_userprop_datasets {

	typeset LABEL=$1
	export ALL=/tmp/zfs-auto-snapshot-list.$$
	export EXCLUDE=/tmp/zfs-auto-snapshot-exclude.$$

  	zfs list -H -t filesystem,volume -o \
	name,com.sun:auto-snapshot,com.sun:auto-snapshot:${LABEL} > $ALL
	cat $ALL | egrep -e "false$"\|"false	-$" > $EXCLUDE

	# iterating through datasets
	for ds in $(cat $ALL | cut -f1 | sort -u) ; do
        	if can_recursive_snapshot $ds ; then
                	print_note "OK to recursive snapshot $ds"
                	RECURSIVE_LIST="${RECURSIVE_LIST} $ds"
        	else
                	if ! is_excluded $ds ; then
                        	print_note "OK to snapshot sole dataset $ds"
                        	SINGLE_LIST="${SINGLE_LIST} $ds"
                	else
                        	print_note "$ds will not be snapshotted"
                	fi
        	fi
	done

	FINAL_RECURSIVE_LIST=$(narrow_recursive_filesystems $RECURSIVE_LIST)
	print_note "Narrowed list of datasets to recursively snapshot is"
	print_note "$FINAL_RECURSIVE_LIST"
	export RECURSIVE_LIST="$FINAL_RECURSIVE_LIST"
	export SINGLE_LIST

	rm $ALL
	rm $EXCLUDE
}

# Determine if a pool is currently being scrubbed or resilvered.
# Return 0 if it is scrubbing/resilvering, 1 otherwise.

# The 2nd arg is a cache of pools known to be not scrubbing during this
# invocation of the script. This does risk a scrub starting mid-way through
# the script and us not checking for it - but if that's just
# happened, then restarting the scrub as a result of a snapshot being taken
# won't be too expensive.
function is_scrubbing { # POOL SCRUBLIST
	
	typeset POOL=$1
	typeset NOSCRUBLIST="$2"
	typeset SCRUBBING=""

	# see if we can avoid running zpool status, by checking for
	# the pool name in a known list of pools that were not scrubbing
	# the last time we checked.
	echo "$NOSCRUBLIST" | grep "$POOL " > /dev/null
	if [ $? -eq 0 ] ; then
	    return 1
	fi

	SCRUBBING=$(env LC_ALL=C zpool status $POOL | grep " in progress")
	if [ -z "$SCRUBBING" ] ; then
	    return 1
	else
	    return 0
	fi
}

# This function runs on startup - by default, if we're taking snapshots 
# under a // schedule, and there isn't a property set on the pool
# com.sun:auto-snapshot=false, then we set the property to true, causing
# all datasets on the system to get included by the service. 
function auto_include {
	FS_NAME=$fs_name
	LABEL=$label
	if [ "$FS_NAME" == "//" ] ; then
		POOLS=$(zpool list -H -o name)
		for pool in $POOLS ; do
			if ! zpool status -x $pool | grep "state: UNAVAIL" > /dev/null ; then
				SNAPALL=$(zfs get -H -o value com.sun:auto-snapshot $pool)
				SNAPLABEL=$(zfs get -H -o value com.sun:auto-snapshot:$LABEL $pool)
				SNAP=$SNAPALL$SNAPLABEL
				case $SNAP in
				*true | true*)
					;;
				*false | false*)
					;;
				*)
					$PFZFS set com.sun:auto-snapshot=true $pool
					;;
				esac
			fi
		done	
	fi
}



# Here's the beginning of the main script. As we're a method script for SMF,
# we take start and stop arguments, and assume that the $SMF_FMRI value is being
# set. For start and stop, our task is to create a cron job that will take a
# snapshot of the specified filesystem.
#
# Without start | stop arguments, we assume we're being called from the cron job
# created above, where the argument is the FMRI containing properties we can
# consult to in order to actually take the snapshot.

if [ -n "${SMF_FMRI}" ] ; then
	zfs_smf_props $SMF_FMRI
	export LOG=$logfile
fi

# $1 start | stop | refresh | an FMRI that we want to take snapshots of.
case "$1" in
'start')
	auto_include $SMF_FMRI
	schedule_snapshots $SMF_FMRI
	if [ $? -eq 0 ] ; then
	    result=$SMF_EXIT_OK
	else
	    print_log "Problem taking snapshots for $SMF_FMRI"
	    result=$SMF_EXIT_ERR_FATAL
	fi
        ;;

'stop')
	unschedule_snapshots $SMF_FMRI
	if [ $? -eq 0 ] ; then
	    result=$SMF_EXIT_OK
	else
	    print_log "Problem taking snapshots for $SMF_FMRI"
	    result=$SMF_EXIT_ERR_FATAL
	fi
        ;;
# the default case, we actually call from the cron job itself that's
# executing this script, and do the job of taking snapshots.
*)
	SMF_FMRI=$1
	# are we being called with the correct argument (an FMRI) ?

	case $SMF_FMRI in
		svc:/*)
			take_snapshot $SMF_FMRI
			if [ $? -eq 0 ] ; then
			    result=$SMF_EXIT_OK
			else
			    result=$SMF_EXIT_ERR_FATAL
			fi
			;;
		*)
			# not logging these messages - we're assuming a curious
			# user has run the script from the command line.
			echo "Usage from SMF : zfs-auto-snapshot [start | stop]"
			echo "Usage from cron: zfs-auto-snapshot svc:/system/filesystem/zfs/auto-snapshot:instance"
			result=$SMF_EXIT_ERR_FATAL
			;;
		esac			
	;;

esac

exit $result
